This repo contains an example of a simple virtual assistant, written in Python 3. 
This assistant has two functions: search papers on arXiv.org or just chit-chat.

### What is it
In the current state, the assistant can be deployed on Heroku.com and run as a bot in Telegram.

The bot consists of several blocks:
 - `telegram.py` uses `Flask`, `pyTelegramBotAPI` and `pymongo` to make the bot actually run somewhere.
 - `nlu.py` uses context-free grammars and machine learning models convert messages from users into semantic frames 
 (dicts with intents and slots).
 - `business_logic.py` contains the code to actually search on arXiv and present the results.
 - `conversation.py` contains the model for chit-chat, trained on a part of Persona dataset.
 - `train.py` trains the models that will be used in `nlu.py`.

To run the current version of the bot, you need to register your own bot in Telegram (via @botfather).
After that, you can run the bot on your local machine with 
```
pip install -r requirements.txt
export TOKEN=<the token for your bot that you got from @botfather>
python telegram.py --poll
```
Alternatively, you can register a free app and deploy the bot on [Heroku](http://heroku.com).

### What is inside
Grammars, mostly. Because I had no relevant search requests (much less labelled ones) to train on, I had to generate
them by myself. And that's when CFGs come really useful: they allow to generate texts, intents and tags 
at the same time, using hierarchies of patterns.

So the NLU part works as follows:
1. First, we try to match the text with grammars - and in case of success, use labels from them.
2. If the grammars failed, we use neural classifier (the basic model is CNN over with Spacy word embeddings)
to predict the intent.
3. If the predicted intent is `find`, it is then parsed with a neural (bidirectional RNN) tagger to extract 
various slots (paper name/topic/author etc).
4. If the predicted intent is `gc` (stands for General Conversation), the response is generated by the corresponding
model. Otherwise, the response is generated by simple rules.

### What you can improve: search functionality
Currently the bot uses only arXiv API. 
You can add SemanticScholar API to get more information about topics, citations and popularity of papers.

Some possible improvements are:
* create new intents: "find similar papers", "get references from this paper", "get citations of this paper", 
"find papers with the same authors", etc.
* improve the work with "best" (=most popular) articles: currently it just reranks the recent (or most relevant) 
articles by a rough estimate of number of citations.
* work better with synonyms. The arXiv API can search only by exact words, so you probably have to expand the query.


### What you can improve: general conversation
Currently the conversation model is very primitive. It uses only one previous phrase as a context, 
and its reply is just one of the training replies, with the most similar context. That is, only contexts are matched.
As a result, many of the replies are inadequate.

You can deal with it by (in order of increasing difficulty):
    * filtering the training responses in some smart way (to get rid of the too specific ones);
    * train a DSSM-like model that matches context with response;
    * train a seq2seq model to generate responses from scratch.
